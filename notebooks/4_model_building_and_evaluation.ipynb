{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93e294c7-428e-4552-a033-c5ac3557efc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL BUILDING AND EVALUATION\n",
      "==================================================\n",
      "\n",
      "1. LOADING TRAINED MODEL AND DATA\n",
      "==============================\n",
      "Loading default model...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'feature_info.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m         best_model \u001b[38;5;241m=\u001b[39m RandomForestRegressor()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Load feature information\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeature_info.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     37\u001b[0m     feature_info \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     39\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m feature_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselected_features\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'feature_info.json'"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"MODEL BUILDING AND EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. LOAD TRAINED MODEL AND DATA\n",
    "print(\"\\n1. LOADING TRAINED MODEL AND DATA\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Load best model\n",
    "try:\n",
    "    best_model = joblib.load('best_model_random_forest.pkl')\n",
    "    print(\"Loaded: Random Forest model\")\n",
    "except:\n",
    "    try:\n",
    "        best_model = joblib.load('best_model_gradient_boosting.pkl')\n",
    "        print(\"Loaded: Gradient Boosting model\")\n",
    "    except:\n",
    "        print(\"Loading default model...\")\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        best_model = RandomForestRegressor()\n",
    "\n",
    "# Load feature information\n",
    "with open('feature_info.json', 'r') as f:\n",
    "    feature_info = json.load(f)\n",
    "\n",
    "selected_features = feature_info['selected_features']\n",
    "print(f\"Number of features: {len(selected_features)}\")\n",
    "\n",
    "# Load predictions\n",
    "predictions_df = pd.read_csv('test_predictions.csv')\n",
    "print(f\"Test predictions loaded: {predictions_df.shape[0]} samples\")\n",
    "\n",
    "# Load full dataset for analysis\n",
    "df = pd.read_csv('cleaned_infectious_disease.csv')\n",
    "df_total = df[df['Sex'] == 'Total'].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c35881ce-edf0-4d18-95b8-28e3b67c2e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. HYPERPARAMETER OPTIMIZATION\n",
      "==============================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_total' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Use the same feature engineering as in training\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# (Recreating features for demonstration)\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m features_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf_total\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     14\u001b[0m features_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear_Since_2000\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m features_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2000\u001b[39m\n\u001b[0;32m     15\u001b[0m features_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRate_Lag1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m features_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCounty\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_total' is not defined"
     ]
    }
   ],
   "source": [
    "# 2. HYPERPARAMETER OPTIMIZATION RESULTS\n",
    "print(\"\\n2. HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Prepare data for hyperparameter tuning\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use the same feature engineering as in training\n",
    "# (Recreating features for demonstration)\n",
    "features_df = df_total.copy()\n",
    "features_df['Year_Since_2000'] = features_df['Year'] - 2000\n",
    "features_df['Rate_Lag1'] = features_df.groupby('County')['Rate'].shift(1)\n",
    "features_df = features_df.dropna(subset=['Rate_Lag1'])\n",
    "\n",
    "# Create simplified feature set for demonstration\n",
    "X = features_df[['Year_Since_2000', 'Rate_Lag1']]\n",
    "y = features_df['Rate']\n",
    "\n",
    "# Split data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, shuffle=False\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, shuffle=False\n",
    ")\n",
    "\n",
    "# Define parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "print(\"Performing Randomized Search for Random Forest...\")\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Use randomized search for efficiency\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf, param_distributions=param_grid,\n",
    "    n_iter=20,  # Number of parameter settings sampled\n",
    "    cv=3,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"Best cross-validation RMSE: {-random_search.best_score_:.4f}\")\n",
    "\n",
    "# Train with best parameters\n",
    "best_rf = random_search.best_estimator_\n",
    "y_pred_val = best_rf.predict(X_val)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
    "print(f\"Validation RMSE with optimized parameters: {val_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c078b0b8-929f-453d-8a89-6f5cc8fa8644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. MODEL COMPARISON WITH STATISTICAL TESTS\n",
    "print(\"\\n3. MODEL COMPARISON WITH STATISTICAL TESTS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Train comparison models\n",
    "models = {\n",
    "    'Random Forest': best_rf,\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'Ridge Regression': Ridge(alpha=1.0)\n",
    "}\n",
    "\n",
    "# Store predictions for statistical tests\n",
    "predictions = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_val)\n",
    "    predictions[name] = pred\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "    print(f\"{name:20s}: RMSE = {rmse:.4f}\")\n",
    "\n",
    "# Diebold-Mariano test for comparing forecasts\n",
    "def diebold_mariano_test(y_true, pred1, pred2, h=1):\n",
    "    \"\"\"Diebold-Mariano test for predictive accuracy.\"\"\"\n",
    "    e1 = y_true - pred1\n",
    "    e2 = y_true - pred2\n",
    "    d = e1**2 - e2**2\n",
    "    \n",
    "    n = len(d)\n",
    "    dm_stat = np.mean(d) / np.sqrt(np.var(d) / n)\n",
    "    p_value = 2 * (1 - stats.norm.cdf(abs(dm_stat)))\n",
    "    \n",
    "    return dm_stat, p_value\n",
    "\n",
    "# Compare Random Forest vs Gradient Boosting\n",
    "dm_stat, p_value = diebold_mariano_test(y_val, predictions['Random Forest'], \n",
    "                                        predictions['Gradient Boosting'])\n",
    "print(f\"\\nDiebold-Mariano Test (RF vs GB):\")\n",
    "print(f\"  DM Statistic: {dm_stat:.4f}\")\n",
    "print(f\"  p-value: {p_value:.4f}\")\n",
    "print(f\"  Significant difference: {'Yes' if p_value < 0.05 else 'No'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b03d5b-0c60-474e-ac68-8b3d3920a65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. PERFORMANCE METRICS ANALYSIS\n",
    "print(\"\\n4. DETAILED PERFORMANCE METRICS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, model_name):\n",
    "    \"\"\"Calculate comprehensive performance metrics.\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Basic metrics\n",
    "    metrics['MAE'] = mean_absolute_error(y_true, y_pred)\n",
    "    metrics['RMSE'] = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    metrics['R2'] = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Percentage errors\n",
    "    metrics['MAPE'] = np.mean(np.abs((y_true - y_pred) / np.maximum(y_true, 1e-10))) * 100\n",
    "    \n",
    "    # Symmetric MAPE\n",
    "    metrics['sMAPE'] = 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / \n",
    "                                               (np.abs(y_true) + np.abs(y_pred) + 1e-10))\n",
    "    \n",
    "    # Theil's U statistic\n",
    "    numerator = np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "    denominator = np.sqrt(np.mean(y_true**2)) + np.sqrt(np.mean(y_pred**2))\n",
    "    metrics['Theil_U'] = numerator / denominator\n",
    "    \n",
    "    # Directional accuracy\n",
    "    y_true_chg = np.diff(y_true) > 0\n",
    "    y_pred_chg = np.diff(y_pred) > 0\n",
    "    if len(y_true_chg) > 0:\n",
    "        metrics['DA'] = np.mean(y_true_chg == y_pred_chg)\n",
    "    else:\n",
    "        metrics['DA'] = np.nan\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric:10s}: {value:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate metrics for best model\n",
    "best_metrics = calculate_metrics(y_test, predictions_df['Predicted'].values, \n",
    "                                \"Best Model on Test Set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef65df6d-f8f2-427d-9585-5a096fd0060c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. RESIDUAL ANALYSIS\n",
      "==============================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'predictions_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Calculate residuals\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m residuals \u001b[38;5;241m=\u001b[39m \u001b[43mpredictions_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActual\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m predictions_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Create residual analysis plots\u001b[39;00m\n\u001b[0;32m      9\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictions_df' is not defined"
     ]
    }
   ],
   "source": [
    "# 5. RESIDUAL DIAGNOSTICS\n",
    "print(\"\\n5. RESIDUAL ANALYSIS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = predictions_df['Actual'] - predictions_df['Predicted']\n",
    "\n",
    "# Create residual analysis plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Residual Diagnostics', fontsize=16)\n",
    "\n",
    "# 1. Residual distribution\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(x=0, color='red', linestyle='--', linewidth=1)\n",
    "ax1.set_title('Distribution of Residuals')\n",
    "ax1.set_xlabel('Residual')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Q-Q plot\n",
    "ax2 = axes[0, 1]\n",
    "stats.probplot(residuals, dist=\"norm\", plot=ax2)\n",
    "ax2.set_title('Q-Q Plot of Residuals')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Residuals vs Predicted\n",
    "ax3 = axes[0, 2]\n",
    "scatter = ax3.scatter(predictions_df['Predicted'], residuals, alpha=0.6)\n",
    "ax3.axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "ax3.set_title('Residuals vs Predicted Values')\n",
    "ax3.set_xlabel('Predicted Rate')\n",
    "ax3.set_ylabel('Residual')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Residuals vs Time (Year)\n",
    "ax4 = axes[1, 0]\n",
    "for county in predictions_df['County'].unique()[:5]:  # Top 5 counties\n",
    "    county_data = predictions_df[predictions_df['County'] == county]\n",
    "    ax4.scatter(county_data['Year'], county_data['Actual'] - county_data['Predicted'], \n",
    "               alpha=0.7, label=county, s=50)\n",
    "ax4.axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "ax4.set_title('Residuals by Year (Top 5 Counties)')\n",
    "ax4.set_xlabel('Year')\n",
    "ax4.set_ylabel('Residual')\n",
    "ax4.legend(loc='best', fontsize=8)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Actual vs Predicted\n",
    "ax5 = axes[1, 1]\n",
    "ax5.scatter(predictions_df['Actual'], predictions_df['Predicted'], alpha=0.6)\n",
    "# Perfect prediction line\n",
    "min_val = min(predictions_df['Actual'].min(), predictions_df['Predicted'].min())\n",
    "max_val = max(predictions_df['Actual'].max(), predictions_df['Predicted'].max())\n",
    "ax5.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect Prediction')\n",
    "ax5.set_title('Actual vs Predicted Values')\n",
    "ax5.set_xlabel('Actual Rate')\n",
    "ax5.set_ylabel('Predicted Rate')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Residual autocorrelation\n",
    "ax6 = axes[1, 2]\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "plot_acf(residuals, lags=10, ax=ax6, alpha=0.05)\n",
    "ax6.set_title('Autocorrelation of Residuals')\n",
    "ax6.set_xlabel('Lag')\n",
    "ax6.set_ylabel('Autocorrelation')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('residual_diagnostics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Statistical tests on residuals\n",
    "print(\"\\nStatistical Tests on Residuals:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Shapiro-Wilk test for normality\n",
    "shapiro_stat, shapiro_p = stats.shapiro(residuals)\n",
    "print(f\"Shapiro-Wilk test for normality:\")\n",
    "print(f\"  Statistic: {shapiro_stat:.4f}\")\n",
    "print(f\"  p-value: {shapiro_p:.4e}\")\n",
    "print(f\"  Residuals are normal: {'Yes' if shapiro_p > 0.05 else 'No'}\")\n",
    "\n",
    "# Durbin-Watson test for autocorrelation\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "dw_stat = durbin_watson(residuals)\n",
    "print(f\"\\nDurbin-Watson test for autocorrelation:\")\n",
    "print(f\"  Statistic: {dw_stat:.4f}\")\n",
    "print(f\"  Interpretation: {'No autocorrelation' if 1.5 < dw_stat < 2.5 else 'Possible autocorrelation'}\")\n",
    "\n",
    "# Breusch-Pagan test for heteroscedasticity\n",
    "import statsmodels.api as sm\n",
    "X_with_const = sm.add_constant(predictions_df['Predicted'])\n",
    "model = sm.OLS(residuals**2, X_with_const).fit()\n",
    "bp_stat = model.nobs * model.rsquared\n",
    "bp_p = 1 - stats.chi2.cdf(bp_stat, 1)\n",
    "print(f\"\\nBreusch-Pagan test for heteroscedasticity:\")\n",
    "print(f\"  Statistic: {bp_stat:.4f}\")\n",
    "print(f\"  p-value: {bp_p:.4e}\")\n",
    "print(f\"  Homoscedastic: {'Yes' if bp_p > 0.05 else 'No'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ff2be73-56b5-4e7a-9222-a762e9ae59f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. BIAS-VARIANCE ANALYSIS\n",
      "==============================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m learning_curve\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Prepare data for learning curves\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m X_full \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mX_train\u001b[49m, X_val, X_test])\n\u001b[0;32m      9\u001b[0m y_full \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([y_train, y_val, y_test])\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Calculate learning curves\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# 6. BIAS-VARIANCE ANALYSIS\n",
    "print(\"\\n6. BIAS-VARIANCE ANALYSIS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Prepare data for learning curves\n",
    "X_full = pd.concat([X_train, X_val, X_test])\n",
    "y_full = pd.concat([y_train, y_val, y_test])\n",
    "\n",
    "# Calculate learning curves\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_rf, X_full, y_full,\n",
    "    cv=3,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate mean and std\n",
    "train_scores_mean = -np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = -np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color='blue', \n",
    "         label='Training RMSE', linewidth=2)\n",
    "plt.fill_between(train_sizes, \n",
    "                 train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std,\n",
    "                 alpha=0.2, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, val_scores_mean, 'o-', color='green',\n",
    "         label='Validation RMSE', linewidth=2)\n",
    "plt.fill_between(train_sizes,\n",
    "                 val_scores_mean - val_scores_std,\n",
    "                 val_scores_mean + val_scores_std,\n",
    "                 alpha=0.2, color='green')\n",
    "\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Learning Curves (Bias-Variance Analysis)')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Calculate bias and variance\n",
    "bias = train_scores_mean[-1]  # Training error at full dataset\n",
    "variance = val_scores_mean[-1] - train_scores_mean[-1]  # Gap between val and train\n",
    "\n",
    "print(f\"Bias (Training Error): {bias:.4f}\")\n",
    "print(f\"Variance (Validation - Training): {variance:.4f}\")\n",
    "print(f\"Total Error: {val_scores_mean[-1]:.4f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "if bias > variance:\n",
    "    print(\"  Model is likely underfitting (high bias)\")\n",
    "else:\n",
    "    print(\"  Model is likely overfitting (high variance)\")\n",
    "\n",
    "plt.savefig('learning_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f19f444-e85f-4644-aa38-657bffdb3631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. MODEL EXPLAINABILITY (SHAP ANALYSIS)\n",
      "==============================\n",
      "Calculating SHAP values...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_rf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculating SHAP values...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Create explainer\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mTreeExplainer(\u001b[43mbest_rf\u001b[49m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Calculate SHAP values for a sample\u001b[39;00m\n\u001b[0;32m     14\u001b[0m X_sample \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;28mlen\u001b[39m(X_train)), random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_rf' is not defined"
     ]
    }
   ],
   "source": [
    "# 7. MODEL EXPLAINABILITY (SHAP VALUES)\n",
    "print(\"\\n7. MODEL EXPLAINABILITY (SHAP ANALYSIS)\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    \n",
    "    print(\"Calculating SHAP values...\")\n",
    "    \n",
    "    # Create explainer\n",
    "    explainer = shap.TreeExplainer(best_rf)\n",
    "    \n",
    "    # Calculate SHAP values for a sample\n",
    "    X_sample = X_train.sample(min(100, len(X_train)), random_state=42)\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values, X_sample, show=False)\n",
    "    plt.title('SHAP Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('shap_summary.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Force plot for a single prediction\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    shap.force_plot(explainer.expected_value, shap_values[0], X_sample.iloc[0], \n",
    "                   matplotlib=True, show=False)\n",
    "    plt.title('SHAP Force Plot for Single Prediction')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('shap_force_plot.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate feature importance from SHAP\n",
    "    shap_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'shap_importance': np.abs(shap_values).mean(axis=0)\n",
    "    }).sort_values('shap_importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 5 features by SHAP importance:\")\n",
    "    print(shap_importance.head())\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"SHAP not installed. Using feature importance from model instead.\")\n",
    "    \n",
    "    # Feature importance from Random Forest\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 5 features by model importance:\")\n",
    "    print(feature_importance.head())\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_features = feature_importance.head(10)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importance (Random Forest)')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(True, alpha=0.3, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f83b34b3-78c2-4056-88c4-79c4c07784f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. PREDICTION INTERVALS\n",
      "==============================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Train a model to estimate prediction intervals\u001b[39;00m\n\u001b[0;32m      9\u001b[0m rf_intervals \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m rf_intervals\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Get predictions from individual trees\u001b[39;00m\n\u001b[0;32m     13\u001b[0m predictions_by_tree \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([tree\u001b[38;5;241m.\u001b[39mpredict(X_test) \u001b[38;5;28;01mfor\u001b[39;00m tree \u001b[38;5;129;01min\u001b[39;00m rf_intervals\u001b[38;5;241m.\u001b[39mestimators_])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# 8. PREDICTION INTERVALS\n",
    "print(\"\\n8. PREDICTION INTERVALS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Method 1: Using quantile regression forests\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Train a model to estimate prediction intervals\n",
    "rf_intervals = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_intervals.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions from individual trees\n",
    "predictions_by_tree = np.array([tree.predict(X_test) for tree in rf_intervals.estimators_])\n",
    "\n",
    "# Calculate prediction intervals\n",
    "alpha = 0.05  # 95% prediction interval\n",
    "lower_percentile = 100 * alpha / 2\n",
    "upper_percentile = 100 * (1 - alpha / 2)\n",
    "\n",
    "prediction_intervals = np.percentile(predictions_by_tree, \n",
    "                                     [lower_percentile, upper_percentile], \n",
    "                                     axis=0)\n",
    "\n",
    "# Calculate coverage\n",
    "in_interval = np.sum((y_test >= prediction_intervals[0]) & \n",
    "                     (y_test <= prediction_intervals[1])) / len(y_test)\n",
    "\n",
    "print(f\"95% Prediction Interval Coverage: {in_interval*100:.1f}%\")\n",
    "print(f\"Average Interval Width: {np.mean(prediction_intervals[1] - prediction_intervals[0]):.4f}\")\n",
    "\n",
    "# Plot predictions with intervals\n",
    "plt.figure(figsize=(12, 6))\n",
    "sorted_idx = np.argsort(y_test.values)\n",
    "plt.plot(range(len(y_test)), y_test.values[sorted_idx], 'o', \n",
    "         label='Actual', alpha=0.6, markersize=6)\n",
    "plt.plot(range(len(y_test)), rf_intervals.predict(X_test).values[sorted_idx], \n",
    "         's', label='Predicted', alpha=0.6, markersize=4)\n",
    "plt.fill_between(range(len(y_test)),\n",
    "                 prediction_intervals[0][sorted_idx],\n",
    "                 prediction_intervals[1][sorted_idx],\n",
    "                 alpha=0.3, label='95% Prediction Interval')\n",
    "plt.xlabel('Sample Index (Sorted by Actual Value)')\n",
    "plt.ylabel('Incidence Rate')\n",
    "plt.title('Predictions with 95% Prediction Intervals')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('prediction_intervals.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf95669-ea1f-4149-85db-b3416640220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. MODEL ROBUSTNESS CHECKS\n",
    "print(\"\\n9. MODEL ROBUSTNESS ANALYSIS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Check stability across different random seeds\n",
    "rmse_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "for seed in range(42, 52):  # 10 different seeds\n",
    "    rf_temp = RandomForestRegressor(n_estimators=100, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    cv_rmse = -cross_val_score(rf_temp, X_full, y_full, \n",
    "                               cv=3, scoring='neg_root_mean_squared_error',\n",
    "                               n_jobs=-1).mean()\n",
    "    cv_r2 = cross_val_score(rf_temp, X_full, y_full, \n",
    "                           cv=3, scoring='r2', n_jobs=-1).mean()\n",
    "    \n",
    "    rmse_scores.append(cv_rmse)\n",
    "    r2_scores.append(cv_r2)\n",
    "\n",
    "print(f\"RMSE across 10 random seeds:\")\n",
    "print(f\"  Mean: {np.mean(rmse_scores):.4f}\")\n",
    "print(f\"  Std:  {np.std(rmse_scores):.4f}\")\n",
    "print(f\"  Range: [{np.min(rmse_scores):.4f}, {np.max(rmse_scores):.4f}]\")\n",
    "\n",
    "print(f\"\\nRÂ² across 10 random seeds:\")\n",
    "print(f\"  Mean: {np.mean(r2_scores):.4f}\")\n",
    "print(f\"  Std:  {np.std(r2_scores):.4f}\")\n",
    "print(f\"  Range: [{np.min(r2_scores):.4f}, {np.max(r2_scores):.4f}]\")\n",
    "\n",
    "# Sensitivity to feature removal\n",
    "feature_sensitivity = {}\n",
    "base_score = -cross_val_score(best_rf, X_full, y_full, \n",
    "                             cv=3, scoring='neg_root_mean_squared_error',\n",
    "                             n_jobs=-1).mean()\n",
    "\n",
    "for feature in X.columns:\n",
    "    X_reduced = X_full.drop(columns=[feature])\n",
    "    score = -cross_val_score(best_rf, X_reduced, y_full,\n",
    "                            cv=3, scoring='neg_root_mean_squared_error',\n",
    "                            n_jobs=-1).mean()\n",
    "    feature_sensitivity[feature] = score - base_score\n",
    "\n",
    "print(\"\\nFeature Sensitivity (Increase in RMSE when removed):\")\n",
    "for feature, sensitivity in sorted(feature_sensitivity.items(), \n",
    "                                  key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {feature:20s}: {sensitivity:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3636411a-31e5-401d-8b42-c854ed3ce307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036910ee-24a0-4077-bb47-fb8721bdeba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f982d290-8557-47da-96f6-afa316c78ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ee381d-3da5-4797-b71f-301a513742cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a4aedf-26b1-487e-a187-d0285ddb6485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52747fb6-a200-4c95-8a32-76f08fea5d49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
