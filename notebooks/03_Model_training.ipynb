{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c9813cc-6ae5-41a0-8f4c-2675da7336cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for modeling...\n",
      "Dataset shape: (141777, 14)\n",
      "Total records (excluding gender breakdown): 47259\n",
      "\n",
      " 1. PROBLEM FORMULATION\n",
      "==============================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWe will explore three modeling approaches:\\n\\n1. REGRESSION: Predict incidence rate (Rate) for each county-year combination\\n2. CLASSIFICATION: Predict high-risk counties (above median rate)\\n3. TIME SERIES: Forecast future incidence rates\\n\\nPrimary focus: Regression problem to predict incidence rates\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load cleaned data\n",
    "print(\"Loading data for modeling...\")\n",
    "df = pd.read_csv('cleaned_infectious_disease.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Filter for total population (not gender-specific)\n",
    "df_total = df[df['Sex'] == 'Total'].copy()\n",
    "print(f\"Total records (excluding gender breakdown): {df_total.shape[0]}\")\n",
    "\n",
    "# 1. PROBLEM FORMULATION\n",
    "print(\"\\n 1. PROBLEM FORMULATION\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "\"\"\"\n",
    "We will explore three modeling approaches:\n",
    "\n",
    "1. REGRESSION: Predict incidence rate (Rate) for each county-year combination\n",
    "2. CLASSIFICATION: Predict high-risk counties (above median rate)\n",
    "3. TIME SERIES: Forecast future incidence rates\n",
    "\n",
    "Primary focus: Regression problem to predict incidence rates\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00d3cfa4-5569-47cf-9b67-8a978aca0238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. FEATURE ENGINEERING\n",
      "==============================\n",
      "Final dataset shape: (23856, 78)\n",
      "Feature columns: ['Year_Since_2000', 'Year_Squared', 'Population', 'Population_Percentile', 'Rate_Lag1', 'Rate_Lag2', 'Rate_Lag3', 'Rate_MA2', 'Rate_MA3', 'Rate_Change_Lag1', 'Rate_Pct_Change_Lag1', 'County_Rate_Mean', 'County_Rate_Std', 'Rate_Z_Score', 'Population_Year_Interaction', 'Rate_Lag1_Population', 'County_Alpine', 'County_Amador', 'County_Butte', 'County_Calaveras', 'County_California', 'County_Colusa', 'County_Contra Costa', 'County_Del Norte', 'County_El Dorado', 'County_Fresno', 'County_Glenn', 'County_Humboldt', 'County_Imperial', 'County_Inyo', 'County_Kern', 'County_Kings', 'County_Lake', 'County_Lassen', 'County_Los Angeles', 'County_Madera', 'County_Marin', 'County_Mariposa', 'County_Mendocino', 'County_Merced', 'County_Modoc', 'County_Mono', 'County_Monterey', 'County_Napa', 'County_Nevada', 'County_Orange', 'County_Placer', 'County_Plumas', 'County_Riverside', 'County_Sacramento', 'County_San Benito', 'County_San Bernardino', 'County_San Diego', 'County_San Francisco', 'County_San Joaquin', 'County_San Luis Obispo', 'County_San Mateo', 'County_Santa Barbara', 'County_Santa Clara', 'County_Santa Cruz', 'County_Shasta', 'County_Sierra', 'County_Siskiyou', 'County_Solano', 'County_Sonoma', 'County_Stanislaus', 'County_Sutter', 'County_Tehama', 'County_Trinity', 'County_Tulare', 'County_Tuolumne', 'County_Ventura', 'County_Yolo', 'County_Yuba', 'Cluster_0', 'Cluster_1', 'Cluster_2', 'Cluster_3']\n"
     ]
    }
   ],
   "source": [
    "# 2. FEATURE ENGINEERING\n",
    "print(\"\\n2. FEATURE ENGINEERING\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Create features at county-year level\n",
    "features_df = df_total.copy()\n",
    "\n",
    "# Basic temporal features\n",
    "features_df['Year_Since_2000'] = features_df['Year'] - 2000\n",
    "features_df['Year_Squared'] = features_df['Year_Since_2000'] ** 2\n",
    "\n",
    "# Lag features (previous year's rate)\n",
    "features_df = features_df.sort_values(['County', 'Year'])\n",
    "\n",
    "# Create lagged rate for each county\n",
    "features_df['Rate_Lag1'] = features_df.groupby('County')['Rate'].shift(1)\n",
    "features_df['Rate_Lag2'] = features_df.groupby('County')['Rate'].shift(2)\n",
    "features_df['Rate_Lag3'] = features_df.groupby('County')['Rate'].shift(3)\n",
    "\n",
    "# Moving averages\n",
    "features_df['Rate_MA2'] = features_df.groupby('County')['Rate'].rolling(window=2).mean().reset_index(level=0, drop=True)\n",
    "features_df['Rate_MA3'] = features_df.groupby('County')['Rate'].rolling(window=3).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# Rate change features\n",
    "features_df['Rate_Change_Lag1'] = features_df['Rate'] - features_df['Rate_Lag1']\n",
    "features_df['Rate_Pct_Change_Lag1'] = (features_df['Rate_Change_Lag1'] / features_df['Rate_Lag1']) * 100\n",
    "\n",
    "# County-level statistics (using only past data to avoid data leakage)\n",
    "county_stats = features_df.groupby('County').agg({\n",
    "    'Rate': ['mean', 'std', 'min', 'max']\n",
    "}).round(3)\n",
    "county_stats.columns = ['County_Rate_Mean', 'County_Rate_Std', 'County_Rate_Min', 'County_Rate_Max']\n",
    "\n",
    "# Merge county statistics\n",
    "features_df = features_df.merge(county_stats, left_on='County', right_index=True, how='left')\n",
    "\n",
    "# Create rate z-score relative to county history\n",
    "features_df['Rate_Z_Score'] = (features_df['Rate'] - features_df['County_Rate_Mean']) / features_df['County_Rate_Std']\n",
    "\n",
    "# Population density proxy (if we had area data, we'd use actual density)\n",
    "# Using population percentiles instead\n",
    "population_percentiles = features_df['Population'].rank(pct=True)\n",
    "features_df['Population_Percentile'] = population_percentiles\n",
    "\n",
    "# Create interaction features\n",
    "features_df['Population_Year_Interaction'] = features_df['Population'] * features_df['Year_Since_2000']\n",
    "features_df['Rate_Lag1_Population'] = features_df['Rate_Lag1'] * features_df['Population']\n",
    "\n",
    "# Regional features (group counties by rate patterns)\n",
    "# First, identify county clusters from EDA\n",
    "from sklearn.cluster import KMeans\n",
    "county_features = county_stats.values\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "county_clusters = kmeans.fit_predict(county_features)\n",
    "county_cluster_map = dict(zip(county_stats.index, county_clusters))\n",
    "features_df['County_Cluster'] = features_df['County'].map(county_cluster_map)\n",
    "\n",
    "# Create dummy variables for categorical features\n",
    "county_dummies = pd.get_dummies(features_df['County'], prefix='County', drop_first=True)\n",
    "cluster_dummies = pd.get_dummies(features_df['County_Cluster'], prefix='Cluster')\n",
    "\n",
    "# Combine all features\n",
    "X_full = pd.concat([\n",
    "    features_df[[\n",
    "        'Year_Since_2000', 'Year_Squared', 'Population', 'Population_Percentile',\n",
    "        'Rate_Lag1', 'Rate_Lag2', 'Rate_Lag3', 'Rate_MA2', 'Rate_MA3',\n",
    "        'Rate_Change_Lag1', 'Rate_Pct_Change_Lag1',\n",
    "        'County_Rate_Mean', 'County_Rate_Std', 'Rate_Z_Score',\n",
    "        'Population_Year_Interaction', 'Rate_Lag1_Population'\n",
    "    ]],\n",
    "    county_dummies,\n",
    "    cluster_dummies\n",
    "], axis=1)\n",
    "\n",
    "# Target variable\n",
    "y = features_df['Rate']\n",
    "\n",
    "# Remove rows with missing values (from lag features)\n",
    "valid_mask = ~X_full.isnull().any(axis=1) & ~y.isnull()\n",
    "X_full = X_full[valid_mask]\n",
    "y = y[valid_mask]\n",
    "features_df = features_df[valid_mask]\n",
    "\n",
    "print(f\"Final dataset shape: {X_full.shape}\")\n",
    "print(f\"Feature columns: {list(X_full.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7262fcc6-83a7-461f-a50e-804d3a732486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. FEATURE SELECTION\n",
      "==============================\n",
      "Top 10 features by absolute correlation:\n",
      "Rate_Z_Score            0.898879\n",
      "Rate_Change_Lag1        0.707160\n",
      "Rate_MA2                0.707052\n",
      "Rate_MA3                0.667718\n",
      "County_Rate_Mean        0.050621\n",
      "Cluster_3               0.049963\n",
      "County_Rate_Std         0.047817\n",
      "County_San Francisco    0.037481\n",
      "County_Alpine           0.035517\n",
      "County_Kern             0.031082\n",
      "dtype: float64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Method 2: SelectKBest using f_regression\u001b[39;00m\n\u001b[0;32m     21\u001b[0m selector_kbest \u001b[38;5;241m=\u001b[39m SelectKBest(score_func\u001b[38;5;241m=\u001b[39mf_regression, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m X_kbest \u001b[38;5;241m=\u001b[39m \u001b[43mselector_kbest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_temp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_temp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m selected_features_kbest \u001b[38;5;241m=\u001b[39m X_temp\u001b[38;5;241m.\u001b[39mcolumns[selector_kbest\u001b[38;5;241m.\u001b[39mget_support()]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSelected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(selected_features_kbest)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features using SelectKBest:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:921\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:561\u001b[0m, in \u001b[0;36m_BaseFilter.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    559\u001b[0m     X \u001b[38;5;241m=\u001b[39m validate_data(\u001b[38;5;28mself\u001b[39m, X, accept_sparse\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 561\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params(X, y)\n\u001b[0;32m    566\u001b[0m score_func_ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_func(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2961\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2959\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m   2960\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2961\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:1370\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m     )\n\u001b[0;32m   1368\u001b[0m ensure_all_finite \u001b[38;5;241m=\u001b[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[1;32m-> 1370\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1387\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1389\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:1107\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[1;32m-> 1107\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1116\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# 3. FEATURE SELECTION\n",
    "print(\"\\n3. FEATURE SELECTION\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Split data for feature selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_full, y, test_size=0.2, random_state=42, shuffle=False\n",
    ")\n",
    "\n",
    "# Method 1: Correlation with target\n",
    "correlations = X_temp.corrwith(y_temp).abs().sort_values(ascending=False)\n",
    "print(\"Top 10 features by absolute correlation:\")\n",
    "print(correlations.head(10))\n",
    "\n",
    "# Method 2: SelectKBest using f_regression\n",
    "selector_kbest = SelectKBest(score_func=f_regression, k=20)\n",
    "X_kbest = selector_kbest.fit_transform(X_temp, y_temp)\n",
    "selected_features_kbest = X_temp.columns[selector_kbest.get_support()].tolist()\n",
    "print(f\"\\nSelected {len(selected_features_kbest)} features using SelectKBest:\")\n",
    "\n",
    "# Method 3: Recursive Feature Elimination\n",
    "estimator = LinearRegression()\n",
    "selector_rfe = RFE(estimator, n_features_to_select=15, step=1)\n",
    "selector_rfe.fit(X_temp, y_temp)\n",
    "selected_features_rfe = X_temp.columns[selector_rfe.get_support()].tolist()\n",
    "print(f\"\\nSelected {len(selected_features_rfe)} features using RFE:\")\n",
    "\n",
    "# Combine selection methods\n",
    "selected_features = list(set(selected_features_kbest[:15] + selected_features_rfe))\n",
    "print(f\"\\nTotal unique selected features: {len(selected_features)}\")\n",
    "print(\"Selected features:\")\n",
    "for i, feat in enumerate(sorted(selected_features), 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "\n",
    "# Use selected features\n",
    "X_selected = X_full[selected_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fa1c221-8996-4c03-9779-7c6a234abba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. DATA SPLITTING STRATEGY\n",
      "==============================\n",
      "Training years: [np.int64(2001), np.int64(2002), np.int64(2003), np.int64(2004), np.int64(2005), np.int64(2006), np.int64(2007), np.int64(2008), np.int64(2009), np.int64(2010), np.int64(2011)]\n",
      "Validation years: [np.int64(2012)]\n",
      "Test years: [np.int64(2013), np.int64(2014)]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_selected' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m test_mask \u001b[38;5;241m=\u001b[39m features_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(test_years)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Split data\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mX_selected\u001b[49m[train_mask]\n\u001b[0;32m     25\u001b[0m X_val \u001b[38;5;241m=\u001b[39m X_selected[val_mask]\n\u001b[0;32m     26\u001b[0m X_test \u001b[38;5;241m=\u001b[39m X_selected[test_mask]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_selected' is not defined"
     ]
    }
   ],
   "source": [
    "# 4. TRAIN/VALIDATION/TEST SPLITS\n",
    "print(\"\\n4. DATA SPLITTING STRATEGY\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Time-based splitting (since this is time series data)\n",
    "# Use first 80% of years for training, next 10% for validation, last 10% for testing\n",
    "\n",
    "# Get unique years\n",
    "unique_years = sorted(features_df['Year'].unique())\n",
    "train_years = unique_years[:int(len(unique_years)*0.8)]  # 2001-2010\n",
    "val_years = unique_years[int(len(unique_years)*0.8):int(len(unique_years)*0.9)]  # 2011-2012\n",
    "test_years = unique_years[int(len(unique_years)*0.9):]  # 2013-2014\n",
    "\n",
    "print(f\"Training years: {train_years}\")\n",
    "print(f\"Validation years: {val_years}\")\n",
    "print(f\"Test years: {test_years}\")\n",
    "\n",
    "# Create masks\n",
    "train_mask = features_df['Year'].isin(train_years)\n",
    "val_mask = features_df['Year'].isin(val_years)\n",
    "test_mask = features_df['Year'].isin(test_years)\n",
    "\n",
    "# Split data\n",
    "X_train = X_selected[train_mask]\n",
    "X_val = X_selected[val_mask]\n",
    "X_test = X_selected[test_mask]\n",
    "\n",
    "y_train = y[train_mask]\n",
    "y_val = y[val_mask]\n",
    "y_test = y[test_mask]\n",
    "\n",
    "print(f\"\\nData split sizes:\")\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7446acbf-e85c-448a-8c6b-eeb16ab6b707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. BASELINE MODELS\n",
      "==============================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE\u001b[39m\u001b[38;5;124m'\u001b[39m: mae, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSE\u001b[39m\u001b[38;5;124m'\u001b[39m: rmse, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR2\u001b[39m\u001b[38;5;124m'\u001b[39m: r2}\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Baseline 1: Mean predictor (predict mean of training data)\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m train_mean \u001b[38;5;241m=\u001b[39m \u001b[43my_train\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     24\u001b[0m y_pred_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull_like(y_val, train_mean)\n\u001b[0;32m     25\u001b[0m baseline1_results \u001b[38;5;241m=\u001b[39m evaluate_model(y_val, y_pred_mean, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseline 1: Mean Predictor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "# 5. BASELINE MODELS\n",
    "print(\"\\n5. BASELINE MODELS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"Calculate and display evaluation metrics.\"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  MAE:  {mae:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  RÂ²:   {r2:.4f}\")\n",
    "    print(f\"  MAPE: {np.mean(np.abs((y_true - y_pred) / y_true)) * 100:.2f}%\")\n",
    "    \n",
    "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "# Baseline 1: Mean predictor (predict mean of training data)\n",
    "train_mean = y_train.mean()\n",
    "y_pred_mean = np.full_like(y_val, train_mean)\n",
    "baseline1_results = evaluate_model(y_val, y_pred_mean, \"Baseline 1: Mean Predictor\")\n",
    "\n",
    "# Baseline 2: Last value predictor (for time series)\n",
    "# For each county, use last available rate\n",
    "last_rates = features_df[features_df['Year'].isin(train_years)].groupby('County')['Rate'].last()\n",
    "y_pred_last = features_df[val_mask].apply(\n",
    "    lambda row: last_rates.get(row['County'], train_mean), axis=1\n",
    ")\n",
    "baseline2_results = evaluate_model(y_val, y_pred_last, \"Baseline 2: Last Value Predictor\")\n",
    "\n",
    "# Baseline 3: County mean predictor\n",
    "county_means = features_df[features_df['Year'].isin(train_years)].groupby('County')['Rate'].mean()\n",
    "y_pred_county_mean = features_df[val_mask].apply(\n",
    "    lambda row: county_means.get(row['County'], train_mean), axis=1\n",
    ")\n",
    "baseline3_results = evaluate_model(y_val, y_pred_county_mean, \"Baseline 3: County Mean Predictor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca15d2b9-a40b-46ba-92ae-ca0c035af831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. LINEAR MODELS\n",
      "==============================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Scale features for regularized models\u001b[39;00m\n\u001b[0;32m      9\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m---> 10\u001b[0m X_train_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mX_train\u001b[49m)\n\u001b[0;32m     11\u001b[0m X_val_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_val)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Linear Regression\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# 6. LINEAR MODELS\n",
    "print(\"\\n6. LINEAR MODELS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale features for regularized models\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Linear Regression\n",
    "print(\"\\nLinear Regression:\")\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_val)\n",
    "lr_results = evaluate_model(y_val, y_pred_lr, \"Linear Regression\")\n",
    "\n",
    "# Ridge Regression\n",
    "print(\"\\nRidge Regression:\")\n",
    "ridge = Ridge(alpha=1.0, random_state=42)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "y_pred_ridge = ridge.predict(X_val_scaled)\n",
    "ridge_results = evaluate_model(y_val, y_pred_ridge, \"Ridge Regression\")\n",
    "\n",
    "# Lasso Regression\n",
    "print(\"\\nLasso Regression:\")\n",
    "lasso = Lasso(alpha=0.01, random_state=42, max_iter=10000)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "y_pred_lasso = lasso.predict(X_val_scaled)\n",
    "lasso_results = evaluate_model(y_val, y_pred_lasso, \"Lasso Regression\")\n",
    "\n",
    "# Check feature importance from Lasso\n",
    "lasso_coef = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'coefficient': lasso.coef_\n",
    "}).sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 features by Lasso coefficient magnitude:\")\n",
    "print(lasso_coef.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08d759c5-881f-4f17-b632-e62833503a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. TREE-BASED MODELS\n",
      "==============================\n",
      "\n",
      "Decision Tree:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDecision Tree:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m dt \u001b[38;5;241m=\u001b[39m DecisionTreeRegressor(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m dt\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[0;32m     12\u001b[0m y_pred_dt \u001b[38;5;241m=\u001b[39m dt\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[0;32m     13\u001b[0m dt_results \u001b[38;5;241m=\u001b[39m evaluate_model(y_val, y_pred_dt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecision Tree\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# 7. TREE-BASED MODELS\n",
    "print(\"\\n7. TREE-BASED MODELS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Decision Tree\n",
    "print(\"\\nDecision Tree:\")\n",
    "dt = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_val)\n",
    "dt_results = evaluate_model(y_val, y_pred_dt, \"Decision Tree\")\n",
    "\n",
    "# Random Forest\n",
    "print(\"\\nRandom Forest:\")\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_val)\n",
    "rf_results = evaluate_model(y_val, y_pred_rf, \"Random Forest\")\n",
    "\n",
    "# Gradient Boosting\n",
    "print(\"\\nGradient Boosting:\")\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, \n",
    "                              max_depth=3, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "y_pred_gb = gb.predict(X_val)\n",
    "gb_results = evaluate_model(y_val, y_pred_gb, \"Gradient Boosting\")\n",
    "\n",
    "# Feature importance from Random Forest\n",
    "rf_importance = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 features by Random Forest importance:\")\n",
    "print(rf_importance.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60c8f4a0-70d4-41f6-ac68-91b3335b83cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. ENSEMBLE METHODS\n",
      "==============================\n",
      "\n",
      "Voting Regressor:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 15\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mVoting Regressor:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m voting_reg \u001b[38;5;241m=\u001b[39m VotingRegressor([\n\u001b[0;32m     11\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m, LinearRegression()),\n\u001b[0;32m     12\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mridge\u001b[39m\u001b[38;5;124m'\u001b[39m, Ridge(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)),\n\u001b[0;32m     13\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m'\u001b[39m, RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m))\n\u001b[0;32m     14\u001b[0m ])\n\u001b[1;32m---> 15\u001b[0m voting_reg\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[0;32m     16\u001b[0m y_pred_voting \u001b[38;5;241m=\u001b[39m voting_reg\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[0;32m     17\u001b[0m voting_results \u001b[38;5;241m=\u001b[39m evaluate_model(y_val, y_pred_voting, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVoting Regressor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# 8. ENSEMBLE METHODS\n",
    "print(\"\\n8. ENSEMBLE METHODS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Voting Regressor\n",
    "print(\"\\nVoting Regressor:\")\n",
    "voting_reg = VotingRegressor([\n",
    "    ('lr', LinearRegression()),\n",
    "    ('ridge', Ridge(alpha=1.0)),\n",
    "    ('rf', RandomForestRegressor(n_estimators=50, random_state=42))\n",
    "])\n",
    "voting_reg.fit(X_train, y_train)\n",
    "y_pred_voting = voting_reg.predict(X_val)\n",
    "voting_results = evaluate_model(y_val, y_pred_voting, \"Voting Regressor\")\n",
    "\n",
    "# Stacking Regressor\n",
    "print(\"\\nStacking Regressor:\")\n",
    "stacking_reg = StackingRegressor(\n",
    "    estimators=[\n",
    "        ('ridge', Ridge(alpha=1.0)),\n",
    "        ('rf', RandomForestRegressor(n_estimators=50, random_state=42)),\n",
    "        ('gb', GradientBoostingRegressor(n_estimators=50, random_state=42))\n",
    "    ],\n",
    "    final_estimator=LinearRegression(),\n",
    "    cv=5\n",
    ")\n",
    "stacking_reg.fit(X_train, y_train)\n",
    "y_pred_stacking = stacking_reg.predict(X_val)\n",
    "stacking_results = evaluate_model(y_val, y_pred_stacking, \"Stacking Regressor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab27ea6d-7ba2-496d-b6e7-5c0f5fc1666e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9. NEURAL NETWORK MODELS\n",
      "==============================\n",
      "\n",
      "Multi-layer Perceptron:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 19\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMulti-layer Perceptron:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m mlp \u001b[38;5;241m=\u001b[39m MLPRegressor(\n\u001b[0;32m     10\u001b[0m     hidden_layer_sizes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m32\u001b[39m),\n\u001b[0;32m     11\u001b[0m     activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     18\u001b[0m )\n\u001b[1;32m---> 19\u001b[0m mlp\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train_scaled\u001b[49m, y_train)\n\u001b[0;32m     20\u001b[0m y_pred_mlp \u001b[38;5;241m=\u001b[39m mlp\u001b[38;5;241m.\u001b[39mpredict(X_val_scaled)\n\u001b[0;32m     21\u001b[0m mlp_results \u001b[38;5;241m=\u001b[39m evaluate_model(y_val, y_pred_mlp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLP Regressor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "# 9. NEURAL NETWORKS\n",
    "print(\"\\n9. NEURAL NETWORK MODELS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "try:\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    \n",
    "    print(\"\\nMulti-layer Perceptron:\")\n",
    "    mlp = MLPRegressor(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.001,\n",
    "        batch_size=32,\n",
    "        learning_rate='adaptive',\n",
    "        max_iter=500,\n",
    "        random_state=42\n",
    "    )\n",
    "    mlp.fit(X_train_scaled, y_train)\n",
    "    y_pred_mlp = mlp.predict(X_val_scaled)\n",
    "    mlp_results = evaluate_model(y_val, y_pred_mlp, \"MLP Regressor\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"MLPRegressor not available. Skipping neural network models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad5c0a3f-7d48-4c50-b45a-b4c9c3d8a7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10. CROSS-VALIDATION RESULTS\n",
      "==============================\n",
      "Time Series Cross-Validation Results (Negative RMSE):\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m     X_cv \u001b[38;5;241m=\u001b[39m X_train_scaled\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 27\u001b[0m     X_cv \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\n\u001b[0;32m     29\u001b[0m scores \u001b[38;5;241m=\u001b[39m cross_val_score(model, X_cv, y_train, \n\u001b[0;32m     30\u001b[0m                        cv\u001b[38;5;241m=\u001b[39mtscv, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_root_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     31\u001b[0m                        n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     32\u001b[0m cv_results[name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mscores\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# 10. CROSS-VALIDATION STRATEGIES\n",
    "print(\"\\n10. CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "\n",
    "# Time Series Cross-Validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Evaluate models with time series CV\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=50, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=50, random_state=42)\n",
    "}\n",
    "\n",
    "print(\"Time Series Cross-Validation Results (Negative RMSE):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "cv_results = {}\n",
    "for name, model in models.items():\n",
    "    # Use scaled data for Ridge, original for others\n",
    "    if name == 'Ridge Regression':\n",
    "        X_cv = X_train_scaled\n",
    "    else:\n",
    "        X_cv = X_train\n",
    "    \n",
    "    scores = cross_val_score(model, X_cv, y_train, \n",
    "                           cv=tscv, scoring='neg_root_mean_squared_error',\n",
    "                           n_jobs=-1)\n",
    "    cv_results[name] = -scores.mean()\n",
    "    print(f\"{name:20s}: Mean RMSE = {-scores.mean():.4f} (Â±{scores.std():.4f})\")\n",
    "any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44976e9a-c417-4721-a264-3fcaa83abb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11. MODEL PERFORMANCE COMPARISON\n",
      "==============================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'baseline1_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Collect all results\u001b[39;00m\n\u001b[0;32m      6\u001b[0m all_results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean Predictor\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mbaseline1_results\u001b[49m,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLast Value\u001b[39m\u001b[38;5;124m'\u001b[39m: baseline2_results,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCounty Mean\u001b[39m\u001b[38;5;124m'\u001b[39m: baseline3_results,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinear Regression\u001b[39m\u001b[38;5;124m'\u001b[39m: lr_results,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRidge Regression\u001b[39m\u001b[38;5;124m'\u001b[39m: ridge_results,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLasso Regression\u001b[39m\u001b[38;5;124m'\u001b[39m: lasso_results,\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDecision Tree\u001b[39m\u001b[38;5;124m'\u001b[39m: dt_results,\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m'\u001b[39m: rf_results,\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGradient Boosting\u001b[39m\u001b[38;5;124m'\u001b[39m: gb_results,\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVoting Regressor\u001b[39m\u001b[38;5;124m'\u001b[39m: voting_results,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStacking Regressor\u001b[39m\u001b[38;5;124m'\u001b[39m: stacking_results\n\u001b[0;32m     18\u001b[0m }\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Add MLP results if available\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlp_results\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'baseline1_results' is not defined"
     ]
    }
   ],
   "source": [
    "# 11. MODEL COMPARISON\n",
    "print(\"\\n11. MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Collect all results\n",
    "all_results = {\n",
    "    'Mean Predictor': baseline1_results,\n",
    "    'Last Value': baseline2_results,\n",
    "    'County Mean': baseline3_results,\n",
    "    'Linear Regression': lr_results,\n",
    "    'Ridge Regression': ridge_results,\n",
    "    'Lasso Regression': lasso_results,\n",
    "    'Decision Tree': dt_results,\n",
    "    'Random Forest': rf_results,\n",
    "    'Gradient Boosting': gb_results,\n",
    "    'Voting Regressor': voting_results,\n",
    "    'Stacking Regressor': stacking_results\n",
    "}\n",
    "\n",
    "# Add MLP results if available\n",
    "if 'mlp_results' in locals():\n",
    "    all_results['MLP Regressor'] = mlp_results\n",
    "\n",
    "# Create comparison dataframe\n",
    "results_df = pd.DataFrame(all_results).T\n",
    "results_df = results_df.sort_values('RMSE')\n",
    "\n",
    "print(\"\\nModel Performance Ranking (by RMSE):\")\n",
    "print(\"-\" * 50)\n",
    "print(results_df[['RMSE', 'MAE', 'R2']].round(4))\n",
    "\n",
    "# Visualize model comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "models_sorted = results_df.index.tolist()\n",
    "rmse_sorted = results_df['RMSE'].values\n",
    "\n",
    "bars = plt.barh(range(len(models_sorted)), rmse_sorted, color='steelblue', alpha=0.7)\n",
    "plt.yticks(range(len(models_sorted)), models_sorted)\n",
    "plt.xlabel('RMSE (Lower is Better)')\n",
    "plt.title('Model Performance Comparison (Validation Set)')\n",
    "plt.gca().invert_yaxis()  # Highest RMSE at top\n",
    "\n",
    "# Add value labels\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 0.1, bar.get_y() + bar.get_height()/2, \n",
    "             f'{width:.3f}', ha='left', va='center')\n",
    "\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d426514-6404-4c76-b910-4198979104bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12. SELECTING BEST MODEL\n",
      "==============================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m12. SELECTING BEST MODEL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m best_model_name \u001b[38;5;241m=\u001b[39m \u001b[43mresults_df\u001b[49m\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest model based on validation RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_df\u001b[38;5;241m.\u001b[39mloc[best_model_name,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results_df' is not defined"
     ]
    }
   ],
   "source": [
    "# 12. SELECT BEST MODEL FOR FURTHER EVALUATION\n",
    "print(\"\\n12. SELECTING BEST MODEL\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "best_model_name = results_df.index[0]\n",
    "print(f\"Best model based on validation RMSE: {best_model_name}\")\n",
    "print(f\"Validation RMSE: {results_df.loc[best_model_name, 'RMSE']:.4f}\")\n",
    "print(f\"Validation RÂ²: {results_df.loc[best_model_name, 'R2']:.4f}\")\n",
    "\n",
    "# Retrain best model on combined training + validation data\n",
    "print(f\"\\nRetraining {best_model_name} on combined training + validation data...\")\n",
    "\n",
    "# Combine training and validation sets\n",
    "X_train_val = pd.concat([X_train, X_val])\n",
    "y_train_val = pd.concat([y_train, y_val])\n",
    "\n",
    "# Retrain the best model\n",
    "if best_model_name == 'Random Forest':\n",
    "    best_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "elif best_model_name == 'Gradient Boosting':\n",
    "    best_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, \n",
    "                                          max_depth=3, random_state=42)\n",
    "elif best_model_name == 'Ridge Regression':\n",
    "    best_model = Ridge(alpha=1.0, random_state=42)\n",
    "    # Scale data for Ridge\n",
    "    scaler_full = StandardScaler()\n",
    "    X_train_val_scaled = scaler_full.fit_transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    best_model.fit(X_train_val_scaled, y_train_val)\n",
    "    y_pred_test = best_model.predict(X_test_scaled)\n",
    "else:\n",
    "    # Default to Random Forest if model not explicitly handled\n",
    "    best_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    best_model.fit(X_train_val, y_train_val)\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "if best_model_name != 'Ridge Regression':\n",
    "    best_model.fit(X_train_val, y_train_val)\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate on test set\n",
    "print(f\"\\nFinal Evaluation on Test Set ({test_years}):\")\n",
    "test_results = evaluate_model(y_test, y_pred_test, f\"{best_model_name} (Test Set)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afe512a9-ab19-4fd2-9250-f9f123738f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13. SAVING MODELS AND FEATURES\n",
      "==============================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_model_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Save best model\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m model_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mbest_model_name\u001b[49m\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(best_model, model_filename)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved best model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_model_name' is not defined"
     ]
    }
   ],
   "source": [
    "# 13. SAVE MODELS AND FEATURES\n",
    "print(\"\\n13. SAVING MODELS AND FEATURES\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Save best model\n",
    "model_filename = f'best_model_{best_model_name.lower().replace(\" \", \"_\")}.pkl'\n",
    "joblib.dump(best_model, model_filename)\n",
    "print(f\"Saved best model: {model_filename}\")\n",
    "\n",
    "# Save scaler if used\n",
    "if best_model_name == 'Ridge Regression':\n",
    "    joblib.dump(scaler_full, 'scaler.pkl')\n",
    "    print(\"Saved scaler: scaler.pkl\")\n",
    "\n",
    "# Save feature list\n",
    "feature_info = {\n",
    "    'selected_features': selected_features,\n",
    "    'feature_importance': rf_importance.to_dict('records'),\n",
    "    'data_split': {\n",
    "        'train_years': train_years,\n",
    "        'val_years': val_years,\n",
    "        'test_years': test_years\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('feature_info.json', 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2)\n",
    "print(\"Saved feature information: feature_info.json\")\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': y_pred_test,\n",
    "    'County': features_df.loc[test_mask, 'County'],\n",
    "    'Year': features_df.loc[test_mask, 'Year']\n",
    "})\n",
    "predictions_df.to_csv('test_predictions.csv', index=False)\n",
    "print(\"Saved test predictions: test_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1aa1496-0fe1-4f7e-a438-f9340032ba32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14. MODELING SUMMARY\n",
      "==============================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_model_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m14. MODELING SUMMARY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mbest_model_name\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of Features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(selected_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_model_name' is not defined"
     ]
    }
   ],
   "source": [
    "# 14. MODEL SUMMARY\n",
    "print(\"\\n14. MODELING SUMMARY\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Number of Features: {len(selected_features)}\")\n",
    "print(f\"Training Size: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation Size: {X_val.shape[0]} samples\")\n",
    "print(f\"Test Size: {X_test.shape[0]} samples\")\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  Validation RMSE: {results_df.loc[best_model_name, 'RMSE']:.4f}\")\n",
    "print(f\"  Test RMSE: {test_results['RMSE']:.4f}\")\n",
    "print(f\"  Test RÂ²: {test_results['R2']:.4f}\")\n",
    "\n",
    "print(\"\\nKey Features (from Random Forest):\")\n",
    "for i, row in rf_importance.head(5).iterrows():\n",
    "    print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL TRAINING COMPLETE\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f434659-a26e-44d5-911b-c418510fa3c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636136d5-24b6-4e50-bf8d-47a426fea531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d68e1f-eecb-43ee-b9d6-db90181edbc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd5232f-c85a-4cdb-b56c-7a594bda4f76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa2d850-39aa-4c6e-8273-4ef6b3000ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af5a95f-f24e-4a71-b90a-fa88f4b0568a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
